# Optimizing LLaVA: Measuring Mistral Against LLaMA for Visual Instruction Tuning

## Introduction

In the ever-expanding universe of artificial intelligence, Large Language Models (LLMs) have quickly moved from the shadows into the spotlight, transforming how we interact with technology. These models have mastered not just the art of conversation but have also stepped into the realm of understanding and generating visual content. Imagine AI that not only chats with you but can also interpret and create images based on your discussions. That's where our project steps in, optimizing the Large Language-and-Vision Assistant (LLaVA) by switching its core from LLaMA to Mistral, achieving groundbreaking efficiency without sacrificing performance.

## Methods

Our methodology revolves around a strategic alteration to the Large Language-and-Vision Assistant (LLaVA). Transitioning from LLaMA to Mistral 7B as the foundational language model, we harness Mistral's advancements in efficiency and speed without sacrificing the model's depth of understanding and responsiveness.

<img src="../../assets/llavaarch.PNG" width=400>

The architecture underpinning our model involves a seamless blend of a pretrained visual encoder and the robust linguistic processing power of Mistral. This combination allows for a refined interpretation of both visual and textual data, facilitated by innovative techniques such as sliding window and grouped query attention. The model's training was conducted using a carefully selected subset of language-image instruction datasets, optimized for the most effective and generalizable learning outcomes.


## Results 

## Conclusion
